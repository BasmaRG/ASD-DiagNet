{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from functools import reduce\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pyprind\n",
    "import sys\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "import functools\n",
    "import numpy.ma as ma # for masked arrays\n",
    "import pyprind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035\n",
      "1112\n"
     ]
    }
   ],
   "source": [
    "data_main_path = '/home/taban/autism/paper_autism/acerta-abide/acerta-abide/data/functionals/cpac/filt_global/rois_cc200'#path to time series data\n",
    "flist = os.listdir(data_main_path)\n",
    "print(len(flist))\n",
    "\n",
    "df_labels = pd.read_csv('/home/taban/autism/paper_autism/acerta-abide/acerta-abide/data/phenotypes/Phenotypic_V1_0b_preprocessed1.csv')#path \n",
    "\n",
    "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2:0})\n",
    "print(len(df_labels))\n",
    "\n",
    "labels = {}\n",
    "for row in df_labels.iterrows():\n",
    "    file_id = row[1]['FILE_ID']\n",
    "    y_label = row[1]['DX_GROUP']\n",
    "    if file_id == 'no_filename':\n",
    "        continue\n",
    "    assert(file_id not in labels)\n",
    "    labels[file_id] = y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for computing correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corr_data(filename):\n",
    "    df = pd.read_csv(os.path.join(data_main_path, filename), sep='\\t')\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
    "        m = ma.masked_where(mask == 1, mask)\n",
    "        return ma.masked_where(m, corr).compressed()\n",
    "\n",
    "def get_corr_matrix(filename):\n",
    "    df = pd.read_csv(os.path.join(data_main_path, filename), sep='\\t')\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        return corr\n",
    "\n",
    "def confusion(g_turth,predictions):\n",
    "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
    "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "    sensitivity = (tp)/(tp+fn)\n",
    "    specificty = (tn)/(tn+fp)\n",
    "    return accuracy,sensitivity,specificty\n",
    "\n",
    "def get_regs(samplesnames,regnum):\n",
    "    datas = []\n",
    "    for sn in samplesnames:\n",
    "        datas.append(all_corr[sn][0])\n",
    "    datas = np.array(datas)\n",
    "    avg=[]\n",
    "    for ie in range(datas.shape[1]):\n",
    "        avg.append(np.mean(datas[:,ie]))\n",
    "    avg=np.array(avg)\n",
    "    highs=avg.argsort()[-regnum:][::-1]\n",
    "    lows=avg.argsort()[:regnum][::-1]\n",
    "    regions=np.concatenate((highs,lows),axis=0)\n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper fnuctions for computing correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./correlations_file.pkl'):\n",
    "    pbar=pyprind.ProgBar(len(flist))\n",
    "    all_corr = {}\n",
    "    for f in flist:\n",
    "        \n",
    "        f_split = f.split('_')\n",
    "        if f_split[3] == 'rois':\n",
    "            key = '_'.join(f_split[0:3]) #,f_split[2]])\n",
    "        else:\n",
    "            key = '_'.join(f_split[0:2])\n",
    "        \n",
    "        #lab = key\n",
    "      \n",
    "        lab = get_label(f)\n",
    "        all_corr[f] = (get_corr_data(f), lab)\n",
    "        pbar.update()\n",
    "\n",
    "    print('Corr-computations finished')\n",
    "\n",
    "    pickle.dump(all_corr, open('./correlations_file.pkl', 'wb'))\n",
    "    print('Saving to file finished')\n",
    "\n",
    "else:\n",
    "    all_corr = pickle.load(open('./correlations_file.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for getting the label of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(filename):\n",
    "    f_split = filename.split('_')\n",
    "    if f_split[3] == 'rois':\n",
    "        key = '_'.join(f_split[0:3]) \n",
    "    else:\n",
    "        key = '_'.join(f_split[0:2])\n",
    "    assert (key in labels)\n",
    "    \n",
    "    return labels[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing eigenvalues and eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taban/miniconda3/envs/py35/lib/python3.7/site-packages/numpy/lib/function_base.py:2400: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/taban/miniconda3/envs/py35/lib/python3.7/site-packages/numpy/lib/function_base.py:2401: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eig_data = {}\n",
    "pbar = pyprind.ProgBar(len(flist))\n",
    "for f in flist:  \n",
    "    d = get_corr_matrix(f)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(d)\n",
    "\n",
    "    for ev in eig_vecs.T:\n",
    "        np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "\n",
    "    sum_eigvals = np.sum(np.abs(eig_vals))\n",
    "    # Make a list of (eigenvalue, eigenvector, norm_eigval) tuples\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i], np.abs(eig_vals[i])/sum_eigvals)\n",
    "                 for i in range(len(eig_vals))]\n",
    "\n",
    "    # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    eig_data[f] = {'eigvals':np.array([ep[0] for ep in eig_pairs]),\n",
    "                   'norm-eigvals':np.array([ep[2] for ep in eig_pairs]),\n",
    "                   'eigvecs':[ep[1] for ep in eig_pairs]}\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Eros similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_weights(sub_flist):\n",
    "    num_dim = len(eig_data[flist[0]]['eigvals'])\n",
    "    norm_weights = np.zeros(shape=num_dim)\n",
    "    for f in sub_flist:\n",
    "        norm_weights += eig_data[f]['norm-eigvals'] \n",
    "    return norm_weights\n",
    "\n",
    "def cal_similarity(d1, d2, weights, lim=None):\n",
    "    res = 0.0\n",
    "    if lim is None:\n",
    "        weights_arr = weights.copy()\n",
    "    else:\n",
    "        weights_arr = weights[:lim].copy()\n",
    "        weights_arr /= np.sum(weights_arr)\n",
    "    for i,w in enumerate(weights_arr):\n",
    "        res += w*np.inner(d1[i], d2[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CC200Dataset(Dataset):\n",
    "    def __init__(self, pkl_filename=None, data=None, samples_list=None, \n",
    "                 augmentation=False, aug_factor=1, num_neighbs=5,\n",
    "                 eig_data=None, similarity_fn=None, verbose=False,regs=None):\n",
    "        self.regs=regs\n",
    "        if pkl_filename is not None:\n",
    "            if verbose:\n",
    "                print ('Loading ..!', end=' ')\n",
    "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
    "        elif data is not None:\n",
    "            self.data = data.copy()\n",
    "            \n",
    "        else:\n",
    "            sys.stderr.write('Eigther PKL file or data is needed!')\n",
    "            return \n",
    "\n",
    "        if verbose:\n",
    "            print ('Preprocess..!', end='  ')\n",
    "        if samples_list is None:\n",
    "            self.flist = [f for f in self.data]\n",
    "        else:\n",
    "            self.flist = [f for f in samples_list]\n",
    "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
    "        \n",
    "        current_flist = np.array(self.flist.copy())\n",
    "        current_lab0_flist = current_flist[self.labels == 0]\n",
    "        current_lab1_flist = current_flist[self.labels == 1]\n",
    "        if verbose:\n",
    "            print(' Num Positive : ', len(current_lab1_flist), end=' ')\n",
    "            print(' Num Negative : ', len(current_lab0_flist), end=' ')\n",
    "        \n",
    "        \n",
    "        if augmentation:\n",
    "            self.num_data = aug_factor * len(self.flist)\n",
    "            self.neighbors = {}\n",
    "            pbar = pyprind.ProgBar(len(self.flist))\n",
    "            weights = norm_weights(samples_list)#??\n",
    "            for f in self.flist:\n",
    "                label = self.data[f][1]\n",
    "                candidates = (set(current_lab0_flist) if label == 0 else set(current_lab1_flist))\n",
    "                candidates.remove(f)\n",
    "                eig_f = eig_data[f]['eigvecs']\n",
    "                sim_list = []\n",
    "                for cand in candidates:\n",
    "                    eig_cand = eig_data[cand]['eigvecs']\n",
    "                    sim = similarity_fn(eig_f, eig_cand,weights)\n",
    "                    sim_list.append((sim, cand))\n",
    "                sim_list.sort(key=lambda x: x[0], reverse=True)\n",
    "                self.neighbors[f] = [item[1] for item in sim_list[:num_neighbs]]#list(candidates)#[item[1] for item in sim_list[:num_neighbs]]\n",
    "        \n",
    "        else:\n",
    "            self.num_data = len(self.flist)\n",
    "        if verbose:\n",
    "            print('****** Total: >>   ', self.num_data)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.flist):\n",
    "            fname = self.flist[index]\n",
    "            data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode)    \n",
    "            data = data[self.regs].copy()\n",
    "            label = (self.labels[index],)\n",
    "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
    "        else:\n",
    "            f1 = self.flist[index % len(self.flist)]\n",
    "            d1, y1 = self.data[f1][0], self.data[f1][1]\n",
    "            d1=d1[self.regs]\n",
    "            f2 = np.random.choice(self.neighbors[f1])\n",
    "            d2, y2 = self.data[f2][0], self.data[f2][1]\n",
    "            d2=d2[self.regs]\n",
    "            assert y1 == y2\n",
    "            r = np.random.uniform(low=0, high=1)\n",
    "            label = (y1,)\n",
    "            data = r*d1 + (1-r)*d2\n",
    "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definig data loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
    "               batch_size=64, \n",
    "               num_workers=1, mode='train',\n",
    "               *, augmentation=False, aug_factor=1, num_neighbs=5,\n",
    "                 eig_data=None, similarity_fn=None, verbose=False,regions=None):\n",
    "    \"\"\"Build and return data loader.\"\"\"\n",
    "\n",
    "    if mode == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "        augmentation=False\n",
    "        \n",
    "        \n",
    "    if mode=='train':\n",
    "        names=samples_list\n",
    "        datas=[all_corr[i][0] for i in names]\n",
    "        datas=np.array(datas)\n",
    "        avg=[]\n",
    "\n",
    "    dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list,\n",
    "                           augmentation=augmentation, aug_factor=aug_factor, \n",
    "                           eig_data=eig_data, similarity_fn=similarity_fn, verbose=verbose,regs=regions)\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers)\n",
    "  \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTAutoEncoder(\n",
       "  (fc_encoder): Linear(in_features=990, out_features=200, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MTAutoEncoder(nn.Module):\n",
    "    def __init__(self, num_inputs=990, \n",
    "                 num_latent=200, tied=True,\n",
    "                 num_classes=2, use_dropout=False):\n",
    "        super(MTAutoEncoder, self).__init__()\n",
    "        self.tied = tied\n",
    "        self.num_latent = num_latent\n",
    "        \n",
    "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
    "    \n",
    "        if not tied:\n",
    "            self.fc_decoder = nn.Linear(num_latent, num_inputs)\n",
    "         \n",
    "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
    "        \n",
    "        if use_dropout:\n",
    "            self.classifier = nn.Sequential (\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(self.num_latent, 1),\n",
    "                \n",
    "            )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential (\n",
    "                nn.Linear(self.num_latent, 1),\n",
    "            )\n",
    "            \n",
    "         \n",
    "    def forward(self, x, eval_classifier=False):\n",
    "        x = self.fc_encoder(x)\n",
    "        x = torch.tanh(x)\n",
    "        if eval_classifier:\n",
    "            x_logit = self.classifier(x)\n",
    "        else:\n",
    "            x_logit = None\n",
    "        \n",
    "        if self.tied:\n",
    "            x = F.linear(x, self.fc_encoder.weight.t())\n",
    "        else:\n",
    "            x = self.fc_decoder(x)\n",
    "            \n",
    "        return x, x_logit\n",
    "\n",
    "mtae = MTAutoEncoder()\n",
    "\n",
    "mtae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
    "        if len(batch_x) != batch_size:\n",
    "            continue\n",
    "        if p_bernoulli is not None:\n",
    "            if i == 0:\n",
    "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
    "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
    "\n",
    "        data, target = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if mode in ['both', 'ae']:\n",
    "            if p_bernoulli is not None:\n",
    "                rec_noisy, _ = model(data*rand_bernoulli, False)\n",
    "                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n",
    "            else:\n",
    "                rec, _ = model(data, False)\n",
    "                loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
    "\n",
    "        if mode in ['both', 'clf']:\n",
    "            rec_clean, logits = model(data, True)\n",
    "            loss_clf = criterion_clf(logits, target)\n",
    "\n",
    "        if mode == 'both':\n",
    "            loss_total = loss_ae + lam_factor*loss_clf\n",
    "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
    "                                 loss_clf.detach().cpu().numpy()])\n",
    "        elif mode == 'ae':\n",
    "            loss_total = loss_ae\n",
    "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
    "                                 0.0])\n",
    "        elif mode == 'clf':\n",
    "            loss_total = loss_clf\n",
    "            train_losses.append([0.0, \n",
    "                                 loss_clf.detach().cpu().numpy()])\n",
    "\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_losses       \n",
    "\n",
    "def test(model, criterion, test_loader, \n",
    "         eval_classifier=False, num_batch=None):\n",
    "    test_loss, n_test, correct = 0.0, 0, 0\n",
    "    all_predss=[]\n",
    "    if eval_classifier:\n",
    "        y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i,(batch_x,batch_y) in enumerate(test_loader, 1):\n",
    "            if num_batch is not None:\n",
    "                if i >= num_batch:\n",
    "                    continue\n",
    "            data = batch_x.to(device)\n",
    "            rec, logits = model(data, eval_classifier)\n",
    "\n",
    "            test_loss += criterion(rec, data).detach().cpu().numpy() \n",
    "            n_test += len(batch_x)\n",
    "            if eval_classifier:\n",
    "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "                preds = np.ones_like(proba, dtype=np.int32)\n",
    "                preds[proba < 0.5] = 0\n",
    "                all_predss.extend(preds)###????\n",
    "                y_arr = np.array(batch_y, dtype=np.int32)\n",
    "\n",
    "                correct += np.sum(preds == y_arr)\n",
    "                y_true.extend(y_arr.tolist())\n",
    "                y_pred.extend(proba.tolist())\n",
    "        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n",
    "\n",
    "    return  mlp_acc,mlp_sens,mlp_spef,correct/n_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_bernoulli:  None\n",
      "augmentaiton:  True aug_factor:  2 num_neighbs:  5 lim4sim:  2\n",
      "use_dropout:  False \n",
      "\n",
      "Preprocess..!   Num Positive :  454  Num Negative :  477 ****** Total: >>    1862\n",
      "Preprocess..!   Num Positive :  51  Num Negative :  53 ****** Total: >>    104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taban/miniconda3/envs/py35/lib/python3.7/site-packages/torch/cuda/__init__.py:116: UserWarning: \n",
      "    Found GPU1 Quadro 600 which is of cuda capability 2.1.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6826923076923077, 0.6862745098039216, 0.6792452830188679, 0.6826923076923077)\n",
      "(0.6538461538461539, 0.6470588235294118, 0.660377358490566, 0.6538461538461539)\n",
      "(0.75, 0.7843137254901961, 0.7169811320754716, 0.75)\n",
      "(0.6826923076923077, 0.7058823529411765, 0.660377358490566, 0.6826923076923077)\n",
      "(0.6634615384615384, 0.5882352941176471, 0.7358490566037735, 0.6634615384615384)\n",
      "(0.7184466019417476, 0.78, 0.660377358490566, 0.7184466019417476)\n",
      "(0.6699029126213593, 0.66, 0.6792452830188679, 0.6699029126213593)\n",
      "(0.7669902912621359, 0.64, 0.8867924528301887, 0.7669902912621359)\n",
      "(0.7864077669902912, 0.8, 0.7735849056603774, 0.7864077669902912)\n",
      "(0.7475728155339806, 0.62, 0.8679245283018868, 0.7475728155339806)\n",
      "averages:\n",
      "[0.71220127 0.69117647 0.73207547 0.71220127]\n",
      "2117.368711233139\n",
      "Preprocess..!   Num Positive :  454  Num Negative :  477 ****** Total: >>    1862\n",
      "Preprocess..!   Num Positive :  51  Num Negative :  53 ****** Total: >>    104\n",
      "(0.7019230769230769, 0.8431372549019608, 0.5660377358490566, 0.7019230769230769)\n",
      "(0.75, 0.7254901960784313, 0.7735849056603774, 0.75)\n",
      "(0.7403846153846154, 0.7058823529411765, 0.7735849056603774, 0.7403846153846154)\n",
      "(0.7307692307692307, 0.6862745098039216, 0.7735849056603774, 0.7307692307692307)\n",
      "(0.7115384615384616, 0.6470588235294118, 0.7735849056603774, 0.7115384615384616)\n",
      "(0.7087378640776699, 0.64, 0.7735849056603774, 0.7087378640776699)\n",
      "(0.6893203883495146, 0.72, 0.660377358490566, 0.6893203883495146)\n",
      "(0.6990291262135923, 0.72, 0.6792452830188679, 0.6990291262135923)\n",
      "(0.6893203883495146, 0.76, 0.6226415094339622, 0.6893203883495146)\n",
      "(0.7475728155339806, 0.62, 0.8679245283018868, 0.7475728155339806)\n",
      "averages:\n",
      "[0.71453043 0.69898039 0.72924528 0.71453043]\n",
      "4230.932079553604\n",
      "Preprocess..!   Num Positive :  454  Num Negative :  477 ****** Total: >>    1862\n",
      "Preprocess..!   Num Positive :  51  Num Negative :  53 ****** Total: >>    104\n",
      "(0.6730769230769231, 0.5686274509803921, 0.7735849056603774, 0.6730769230769231)\n",
      "(0.7884615384615384, 0.7647058823529411, 0.8113207547169812, 0.7884615384615384)\n",
      "(0.6634615384615384, 0.6862745098039216, 0.6415094339622641, 0.6634615384615384)\n",
      "(0.7403846153846154, 0.6862745098039216, 0.7924528301886793, 0.7403846153846154)\n",
      "(0.6442307692307693, 0.45098039215686275, 0.8301886792452831, 0.6442307692307693)\n",
      "(0.7669902912621359, 0.78, 0.7547169811320755, 0.7669902912621359)\n",
      "(0.7281553398058253, 0.78, 0.6792452830188679, 0.7281553398058253)\n",
      "(0.6407766990291263, 0.58, 0.6981132075471698, 0.6407766990291263)\n",
      "(0.7087378640776699, 0.62, 0.7924528301886793, 0.7087378640776699)\n",
      "(0.6796116504854369, 0.68, 0.6792452830188679, 0.6796116504854369)\n",
      "averages:\n",
      "[0.71081653 0.68588235 0.73459119 0.71081653]\n",
      "6342.477319717407\n",
      "Preprocess..!   Num Positive :  454  Num Negative :  477 ****** Total: >>    1862\n",
      "Preprocess..!   Num Positive :  51  Num Negative :  53 ****** Total: >>    104\n",
      "(0.7115384615384616, 0.7843137254901961, 0.6415094339622641, 0.7115384615384616)\n",
      "(0.6923076923076923, 0.6862745098039216, 0.6981132075471698, 0.6923076923076923)\n",
      "(0.7019230769230769, 0.6470588235294118, 0.7547169811320755, 0.7019230769230769)\n",
      "(0.625, 0.5490196078431373, 0.6981132075471698, 0.625)\n",
      "(0.6634615384615384, 0.6078431372549019, 0.7169811320754716, 0.6634615384615384)\n",
      "(0.6990291262135923, 0.78, 0.6226415094339622, 0.6990291262135923)\n",
      "(0.7087378640776699, 0.72, 0.6981132075471698, 0.7087378640776699)\n",
      "(0.7087378640776699, 0.58, 0.8301886792452831, 0.7087378640776699)\n",
      "(0.7281553398058253, 0.7, 0.7547169811320755, 0.7281553398058253)\n",
      "(0.7475728155339806, 0.74, 0.7547169811320755, 0.7475728155339806)\n",
      "averages:\n",
      "[0.70777399 0.68427451 0.73018868 0.70777399]\n",
      "8460.046698570251\n",
      "Preprocess..!   Num Positive :  454  Num Negative :  477 ****** Total: >>    1862\n",
      "Preprocess..!   Num Positive :  51  Num Negative :  53 ****** Total: >>    104\n",
      "(0.7019230769230769, 0.6078431372549019, 0.7924528301886793, 0.7019230769230769)\n",
      "(0.7019230769230769, 0.7647058823529411, 0.6415094339622641, 0.7019230769230769)\n",
      "(0.7307692307692307, 0.6666666666666666, 0.7924528301886793, 0.7307692307692307)\n",
      "(0.6826923076923077, 0.6862745098039216, 0.6792452830188679, 0.6826923076923077)\n",
      "(0.6826923076923077, 0.7254901960784313, 0.6415094339622641, 0.6826923076923077)\n",
      "(0.7572815533980582, 0.74, 0.7735849056603774, 0.7572815533980582)\n",
      "(0.6504854368932039, 0.48, 0.8113207547169812, 0.6504854368932039)\n",
      "(0.6990291262135923, 0.72, 0.6792452830188679, 0.6990291262135923)\n",
      "(0.8058252427184466, 0.74, 0.8679245283018868, 0.8058252427184466)\n",
      "(0.6893203883495146, 0.6, 0.7735849056603774, 0.6893203883495146)\n",
      "averages:\n",
      "[0.70825803 0.68203922 0.73320755 0.70825803]\n",
      "10574.919446468353\n"
     ]
    }
   ],
   "source": [
    "##########################################################In order to measure time:\n",
    "#######################################################from file dorost bar asase esm va don't do flist shuffle\n",
    "#######################This is just MLP\n",
    "#######################################MESEARING TIME\n",
    "\n",
    "start =time.time()\n",
    "flist = np.array(sorted(os.listdir(data_main_path)))\n",
    "batch_size = 8\n",
    "learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
    "num_epochs = 25\n",
    "\n",
    "p_bernoulli = None\n",
    "augmentation = True\n",
    "use_dropout = False\n",
    "\n",
    "aug_factor = 2\n",
    "num_neighbs = 5\n",
    "lim4sim = 2\n",
    "n_lat = 5000\n",
    "start= time.time()\n",
    "\n",
    "print('p_bernoulli: ', p_bernoulli)\n",
    "print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
    "      'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
    "print('use_dropout: ', use_dropout, '\\n')\n",
    "\n",
    "\n",
    "sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
    "crossval_res_kol=[]\n",
    "y_arr = np.array([get_label(f) for f in flist])\n",
    "\n",
    "kk=0 \n",
    "for rp in range(5):\n",
    "    kf = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    np.random.shuffle(flist)\n",
    "    y_arr = np.array([get_label(f) for f in flist])\n",
    "    for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "\n",
    "        train_samples, test_samples = flist[train_index], flist[test_index]\n",
    "\n",
    "        \n",
    "        verbose = (True if (kk == 0) else False)\n",
    "\n",
    "        regions_inds = get_regs(train_samples,5000)\n",
    "        \n",
    "        num_inpp = len(regions_inds)\n",
    "        n_lat = int(num_inpp/2)\n",
    "        train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
    "                                batch_size=batch_size, mode='train',\n",
    "                                augmentation=augmentation, aug_factor=aug_factor, \n",
    "                                num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function, \n",
    "                                verbose=verbose,regions=regions_inds)\n",
    "\n",
    "        test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
    "                               batch_size=batch_size, mode='test', augmentation=False, \n",
    "                               verbose=verbose,regions=regions_inds)\n",
    "\n",
    "        model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
    "        model.to(device)\n",
    "        criterion_ae = nn.MSELoss(reduction='sum')\n",
    "        criterion_clf = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
    "                               {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
    "                              momentum=0.9)\n",
    "\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            if epoch <= 20:\n",
    "                train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
    "            else:\n",
    "                train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
    "\n",
    "\n",
    "        res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
    "        print(test(model, criterion_ae, test_loader, eval_classifier=True))\n",
    "        crossval_res_kol.append(res_mlp)\n",
    "    print(\"averages:\")\n",
    "    print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
    "    finish= time.time()\n",
    "\n",
    "    print(finish-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
